from langchain_core.messages import trim_messages
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage,AIMessage

from operator import itemgetter
import datetime

from utils.get_prompt import topics_prompt, get_chat_prompt
from utils.get_txt_list import txt_to_list
from utils.reduce_string import get_category

from agents.webiste_expert import WebsiteExpert
from agents.profile_manager import ProfileManager
from agents.location_agent import LocationAgent

class Chatbot:
    def __init__(self, model):
        """
        Initializes the XpathGenie object with the model, session ID, and target element details.
        
        Args:
        - container: Segment with HTML DOM structure containing the element.
        - element_preview: String representation of the element's first tag.
        - relations: Dictionary containing related elements to the target.
        - model: LLM model used for generating responses.
        """
        self.model = model
        # List to save the conversation messages
        self.messages = []
        # Set up session ID and configuration
        # self.session_id = input('Name the session: ')
        self.config = {"configurable": {"session_id": {'abc123'}}}
        # Configures a trimmer to manage long messages in the context by retaining recent messages
        self.trimmer = trim_messages(
            max_tokens=25000,
            strategy="last",
            token_counter=model,
            include_system=True,
            allow_partial=False,
            start_on="human",
        )
        self.topics = txt_to_list("data/topics.txt")
        self.prompt = topics_prompt('prompts/get_topic_prompt.txt',self.topics)
        self.profile_manager = ProfileManager(self.model)
        print('Chatbot Initialized')

    def verify_topics(self):
        print('------ TOPICS CHATBOT CAN HANDLE ------')
        for topic in self.topics:
            print(topic)

    def topic_detector(self, human_input):
        prompt = self.prompt + f"\n {human_input}"
        try:
            topic = self.model.invoke([HumanMessage(content=prompt)])
            print(topic.content)
            if (topic.content in self.topics) or (topic.content == "NO TOPIC"):
                self.topic = topic.content
                return self.topic
            else:
                topic = get_category(topic.content,self.topics)
                self.topic = topic
                return self.topic
        except Exception as e:
            print(f"Topic detector Error: {e}")
    
    def add_message(self, content, is_human=True):
        """
        Adds a message to the conversation history, either from Human or AI.

        Args:
        - content: Message content.
        - is_human: Boolean to determine if it is a human (True) or AI (False) message.
        """
        if is_human:
            self.messages.append(HumanMessage(content=content))
        else:
            self.messages.append(AIMessage(content=content))
    
    def chat(self, input_text, api):
        """
        Processes the interactions, sending the input to the model and returning the response in real-time.
        
        Args:
        - input_text: The feedback agent's message.

        Returns:
        - response_content: The complete content of the message generated by the model.
        """

        # Detect topic based on human input
        topic = self.topic_detector(input_text)
        
        # Add the input text (human message) to the conversation history
        self.add_message(input_text, is_human=True)
        
        # Generate the chat prompt based on the feedback agent's input and context
        chat_prompt = get_chat_prompt(
            file_path='prompts/default_prompt.txt',
            personal_info=self.profile_manager.client_info
        )

        # Create a chain of operations for generating the response
        chain = (
            # Pass the chat messages through a trimmer to handle context size
            RunnablePassthrough.assign(messages=itemgetter("messages") | self.trimmer)
            # Use the chat prompt and model to generate the response
            | chat_prompt
            | self.model
        )
        
        # Prepare an empty response content to accumulate the result
        response_content = ""
        
        if topic == "Consulta de datos":
            # # Stream the model's response in real-time and accumulate it
            # for response in chain.stream(
            #     {
            #         "messages": self.messages
            #     },
            #     config=self.config
            # ):
            #     # Print each part of the response as it is generated
            #     print(response.content, end="", flush=True)
            #     response_content += response.content
            
            # Use the profile manager to act based on the human input
            answer = self.profile_manager.act(input_text)
            # Add the AI response to the conversation history
            self.add_message(answer, is_human=False)

        elif topic == "Ubicacion":
            location_agent = LocationAgent(self.model,input_text)
            answer = location_agent.get_keyword()
        
        elif (topic in self.topics) and (topic != "Consulta de datos") and (topic != "Ubicacion"):

            # # Stream the model's response in real-time and accumulate it
            # for response in chain.stream(
            #     {
            #         "messages": self.messages
            #     },
            #     config=self.config
            # ):
            #     # Print each part of the response as it is generated
            #     print(response.content, end="", flush=True)
            #     response_content += response.content

            # Use WebsiteExpert to generate a response
            web_expert = WebsiteExpert(self.model, topic, self.topics)
            answer = web_expert.answer(input_text, api)
            # Add the AI response to the conversation history
            self.add_message(answer, is_human=False)
        
        else:
            # Stream the model's response in real-time and accumulate it
            for response in chain.stream(
                {
                    "messages": self.messages
                },
                config=self.config
            ):
                # Print each part of the response as it is generated
                # print(response.content, end="", flush=True)
                response_content += response.content
            
            answer = response_content
            # Add AI response to chat history
            self.add_message(answer, is_human=False)

        return answer

    def save_conversation(self):
        """
        Saves the complete conversation to a text file with the date and time in the filename.
        """
        # Get the current timestamp to include in the file name
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        # Define the file path with timestamp and session ID
        file_path = f"conversations/conversation_{timestamp}_{self.session_id}_.txt"
        
        try:
            # Open the file in write mode and save each message
            with open(file_path, 'w') as f:
                for msg in self.messages:
                    # Determine the sender: either "Human" or "AI"
                    sender = "Human" if isinstance(msg, HumanMessage) else "AI"
                    # Write each message to the file
                    f.write(f"{sender}: {msg.content}\n")
            print(f"Conversation successfully saved to {file_path}.")
        except Exception as e:
            print(f"Error saving the conversation: {e}")
